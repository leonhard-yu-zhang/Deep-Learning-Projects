import gin
import tensorflow as tf
import logging
import wandb

from metrics import ConfusionMatrix


@gin.configurable
class Trainer(object):
    def __init__(self, model, ds_train, ds_val, n_classes, run_paths, learning_rate, total_steps, log_interval, ckpt_interval):
        self.model = model
        self.learning_rate = learning_rate
        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)
        self.ds_train = ds_train
        self.ds_val = ds_val
        self.n_classes = n_classes
        self.run_paths = run_paths
        self.total_steps = total_steps
        self.log_interval = log_interval
        self.ckpt_interval = ckpt_interval

        # Loss objective
        # There's no softmax layer in the model, from_logits=True inform the loss function that
        # the output values generated by the model are not normalized.
        # y_true shape: batch_size, y_predict shape: (batch_size, n_classes)
        # normalize the output -> convert integer labels to one-hot encoding -> average loss over a batch
        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

        # Metrics
        self.train_loss = tf.keras.metrics.Mean(name='train_loss')
        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

        self.val_loss = tf.keras.metrics.Mean(name='val_loss')
        self.val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')
        self.confusion_matrix = ConfusionMatrix(n_classes=self.n_classes)

        # Checkpoint Manager
        self.ckpt = tf.train.Checkpoint(optimizer=self.optimizer, model=self.model)
        self.manager = tf.train.CheckpointManager(self.ckpt, self.run_paths["path_ckpts_train"], max_to_keep=5)
        # save the best checkpoint with max val_accuracy
        self.best_val_accuracy = 0
        self.current_val_accuracy = 0
        self.best_confusion_matrix = None
        self.current_confusion_matrix = None

        # Summary Writer for tensorboard
        self.train_log_dir = self.run_paths['path_summary_tensorboard'] + '/train'
        self.val_log_dir = self.run_paths['path_summary_tensorboard']+'/val'
        self.train_summary_writer = tf.summary.create_file_writer(self.train_log_dir)
        self.val_summary_writer = tf.summary.create_file_writer(self.val_log_dir)

    @tf.function
    def train_step(self, images, labels):
        with tf.GradientTape() as tape:
            # training=True is only needed if there are layers with different
            # behavior during training versus inference (e.g. Dropout).
            predictions = self.model(images, training=True)
            loss = self.loss_object(labels, predictions)
        gradients = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

        self.train_loss(loss)
        self.train_accuracy(labels, predictions)

    @tf.function
    def val_step(self, images, labels):
        # training=False is only needed if there are layers with different
        # behavior during training versus inference (e.g. Dropout).
        predictions = self.model(images, training=False)
        v_loss = self.loss_object(labels, predictions)
        # confusion matrix
        self.confusion_matrix.update_state(labels, tf.math.argmax(predictions, axis=1))

        self.val_loss(v_loss)
        self.val_accuracy(labels, predictions)

    def train(self):

        logging.info('\n------------ Starting training ----------')
        self.ckpt.restore(self.manager.latest_checkpoint)
        if self.manager.latest_checkpoint:
            logging.info(f"restored from {self.manager.latest_checkpoint}\n")
        else:
            logging.info("initializing from scratch.\n")

        for idx, (images, labels) in enumerate(self.ds_train):

            step = idx + 1
            # a batch loss is the mean of (num=batch_size) SparseCategoricalCrossentropy losses in a batch
            # in function train_step: train_loss = tf.keras.metrics.Mean(loss) accumulates batch losses of each step.
            # train_loss.result() computes the mean of these (num=log_interval,e.g. 10) batch losses
            # train_accuracy(labels, predictions) accumulates labels and predictions
            # train_loss.result() computes the accuracy of these (num=batch_size*log_interval,e.g. 32*10)
            # (label, prediction) pairs
            self.train_step(images, labels)

            if step % self.log_interval == 0:  # log_interval=10 steps/batches

                # Reset val metrics every log_interval=10 steps
                self.val_loss.reset_states()
                self.val_accuracy.reset_states()
                self.confusion_matrix.reset_states()

                for val_images, val_labels in self.ds_val:  # every log_interval=10 steps, compute the val loss once
                    self.val_step(val_images, val_labels)

                template = 'Step {}, train_loss: {}, train_accuracy: {}, val_loss: {}, val_accuracy: {}, confusion_matrix:\n{}'
                logging.info(template.format(step,
                                             self.train_loss.result(),
                                             self.train_accuracy.result() * 100,
                                             self.val_loss.result(),
                                             self.val_accuracy.result() * 100,
                                             self.confusion_matrix.result())
                             )

                # wandb logging
                wandb.log({'Step': step, 'train_loss': self.train_loss.result(), 'train_accuracy':
                           self.train_accuracy.result() * 100, 'val_loss': self.val_loss.result(),
                           'val_accuracy': self.val_accuracy.result() * 100})

                # Write summary to tensorboard
                with self.train_summary_writer.as_default():
                    tf.summary.scalar('loss', self.train_loss.result(), step=step)
                    tf.summary.scalar('accuracy', self.train_accuracy.result(), step=step)

                with self.val_summary_writer.as_default():
                    tf.summary.scalar('loss', self.val_loss.result(), step=step)
                    tf.summary.scalar('accuracy', self.val_accuracy.result(), step=step)

                # Reset train metrics every log_interval=10 steps
                self.train_loss.reset_states()
                self.train_accuracy.reset_states()

                self.current_val_accuracy = self.val_accuracy.result().numpy()
                self.current_confusion_matrix = self.confusion_matrix.result().numpy()

                yield self.val_accuracy.result().numpy()

            if step % self.ckpt_interval == 0:
                # Save checkpoint
                if self.best_val_accuracy < self.current_val_accuracy:
                    self.best_val_accuracy = self.current_val_accuracy
                    self.best_confusion_matrix = self.current_confusion_matrix

                    # wandb summary
                    wandb.run.summary['best_val_accuracy'] = self.best_val_accuracy*100

                    save_path = self.manager.save()
                    logging.info(f'Saved better checkpoint for step {step}: \n{save_path}.')

            if step % self.total_steps == 0:

                logging.info(f'\nFinished training after {step} steps. '
                             f'Best val_accuracy: {self.best_val_accuracy*100}, '
                             f'confusion_matrix:\n{self.best_confusion_matrix}')

                return self.val_accuracy.result().numpy()
